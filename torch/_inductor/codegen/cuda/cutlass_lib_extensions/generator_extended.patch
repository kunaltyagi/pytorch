diff --git a/python/cutlass_library/generator.py b/python/cutlass_library/generator.py
index 1f07e76b..8f38b9dc 100644
--- a/python/cutlass_library/generator.py
+++ b/python/cutlass_library/generator.py
@@ -184,6 +184,10 @@ def CreateGemmUniversal3xOperator(
   combinations = product(layouts, tile_descriptions, data_types, complex_transforms, schedules, tile_schedulers)
   for layout, tile_description, data_type, complex_transform, schedules, tile_scheduler in combinations:
     kernel_schedule, epilogue_schedule = schedules
+    if tile_description.threadblock_shape[0] < 64:
+      continue
+    if tile_description.threadblock_shape[0]<128 and kernel_schedule == KernelScheduleType.TmaWarpSpecializedCooperative:
+      continue # TmaWarpSpecializedCooperative does not support M<128
     A = TensorDescription(
         data_type["a_type"], layout[0][0], layout[0][1], complex_transform[0])
     B = TensorDescription(
@@ -4340,11 +4344,36 @@ def GenerateSM90_TensorOp_16b_WGMMA_gemm(manifest, cuda_version):
   ]
 
   math_instructions = [
+    MathInstruction(
+      [32, 32, 16],
+      DataType.f16, DataType.f16, DataType.f16,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add),
+    MathInstruction(
+      [64, 64, 16],
+      DataType.f16, DataType.f16, DataType.f16,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add),
     MathInstruction(
       [64, 128, 16],
       DataType.f16, DataType.f16, DataType.f16,
       OpcodeClass.TensorOp,
       MathOperation.multiply_add),
+    MathInstruction(
+      [64, 32, 16],
+      DataType.f16, DataType.f16, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add),
+    MathInstruction(
+      [32, 64, 16],
+      DataType.f16, DataType.f16, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add),
+    MathInstruction(
+      [64, 64, 16],
+      DataType.f16, DataType.f16, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add),
     MathInstruction(
       [64, 128, 16],
       DataType.f16, DataType.f16, DataType.f32,
@@ -4363,6 +4392,15 @@ def GenerateSM90_TensorOp_16b_WGMMA_gemm(manifest, cuda_version):
   for math_inst in math_instructions:
     tile_descriptions_small = [
       # Not compatible with TmaWarpSpecializedCooperative
+      TileDescription(
+        [math_inst.instruction_shape[0], math_inst.instruction_shape[1], math_inst.instruction_shape[2] * 2],
+        0, [4, 1, 1], math_inst, min_cc, max_cc, [1, 1, 1]),
+      TileDescription(
+        [math_inst.instruction_shape[0], math_inst.instruction_shape[1], math_inst.instruction_shape[2] * 4],
+        0, [4, 1, 1], math_inst, min_cc, max_cc, [1, 1, 1]),
+      TileDescription(
+        [math_inst.instruction_shape[0], math_inst.instruction_shape[1], math_inst.instruction_shape[2] * 2],
+        0, [4, 1, 1], math_inst, min_cc, max_cc, [2, 1, 1]),
       TileDescription([math_inst.instruction_shape[0], math_inst.instruction_shape[1], math_inst.instruction_shape[2]*4],
        0, [4, 1, 1], math_inst, min_cc, max_cc, [2,1,1]),
       TileDescription([math_inst.instruction_shape[0], math_inst.instruction_shape[1], math_inst.instruction_shape[2]*4],
@@ -4384,7 +4422,7 @@ def GenerateSM90_TensorOp_16b_WGMMA_gemm(manifest, cuda_version):
       TileDescription([math_inst.instruction_shape[0]*2, math_inst.instruction_shape[1]*2, math_inst.instruction_shape[2]*4],
         0, [4, 2, 1], math_inst, min_cc, max_cc, [1,2,1]),
     ]
-    tile_descriptions = tile_descriptions_medium + tile_descriptions_large
+    tile_descriptions = tile_descriptions_small + tile_descriptions_medium + tile_descriptions_large
 
     data_type = {
       "a_type"   : math_inst.element_a,
@@ -4514,6 +4552,21 @@ def GenerateSM90_TensorOp_16b_WGMMA_alignx_gemm(manifest, cuda_version):
   ]
 
   math_instructions = [
+    MathInstruction(
+      [32, 64, 16],
+      DataType.f16, DataType.f16, DataType.f16,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add),
+    MathInstruction(
+      [64, 32, 16],
+      DataType.f16, DataType.f16, DataType.f16,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add),
+    MathInstruction(
+      [64, 64, 16],
+      DataType.f16, DataType.f16, DataType.f16,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add),
     MathInstruction(
       [64, 128, 16],
       DataType.f16, DataType.f16, DataType.f16,
@@ -4536,14 +4589,17 @@ def GenerateSM90_TensorOp_16b_WGMMA_alignx_gemm(manifest, cuda_version):
 
   for math_inst in math_instructions:
     tile_descriptions_small = [
-      # TileDescription([math_inst.instruction_shape[0], math_inst.instruction_shape[1], math_inst.instruction_shape[2]*4],
-      #   0, [4, 1, 1], math_inst, min_cc, max_cc, [1,1,1]),
+      TileDescription(
+        [math_inst.instruction_shape[0], math_inst.instruction_shape[1], math_inst.instruction_shape[2] * 2],
+        0, [4, 1, 1], math_inst, min_cc, max_cc, [1, 1, 1]),
+      TileDescription([math_inst.instruction_shape[0], math_inst.instruction_shape[1], math_inst.instruction_shape[2]*4],
+         0, [4, 1, 1], math_inst, min_cc, max_cc, [1,1,1]),
     ]
     tile_descriptions_medium = [
       TileDescription([math_inst.instruction_shape[0]*2, math_inst.instruction_shape[1], math_inst.instruction_shape[2]*4],
         0, [4, 1, 1], math_inst, min_cc, max_cc, [1,1,1]),
-      # TileDescription([math_inst.instruction_shape[0], math_inst.instruction_shape[1]*2, math_inst.instruction_shape[2]*4],
-      #   0, [4, 1, 1], math_inst, min_cc, max_cc, [1,1,1]),
+       TileDescription([math_inst.instruction_shape[0], math_inst.instruction_shape[1]*2, math_inst.instruction_shape[2]*4],
+         0, [4, 1, 1], math_inst, min_cc, max_cc, [1,1,1]),
     ]
     tile_descriptions = tile_descriptions_small + tile_descriptions_medium
 
